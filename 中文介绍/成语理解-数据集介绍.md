## 成语理解数据集


## Idiom Comprehension Dataset


### 数据描述：
成语理解数据集采用标准化选择题形式构建，主要包含两大核心题型模块。第一类题型为语境匹配选择题，题目会提供完整的句子语境，其中关键位置预留成语填空，要求从多个选项中精准选出符合上下文语义、语法和逻辑关系的成语。第二类题型聚焦于成语关系辨析，题目会列出若干成语，要求从近义关系、反义关系、感情色彩差异、适用语境区别等维度，判断成语之间的逻辑关联或差异特征。


### 适配方法：
LoRA适配
LoRA微调是指利用低秩分解来表示大模型的参数更新，从而减少微调所需的资源和时间。LORA是Low-Rank Adaptation of Large Language Models的缩写，出自论文《LoRA: Low-Rank Adaptation of Large Language Models》。LORA的基本思想是假设模型在任务适配过程中权重的改变量是低秩（low rank）的，因此可以用两个较小的矩阵来表示参数更新，同时保持预训练的权重不变。LORA可以应用于各种自然语言处理任务，如内容理解、生成任务等，实验表明LORA可以在保持或提高模型性能的同时，显著降低训练参数和推理延迟。


### 数据集构成和规范：

#### 源数据量：

训练集700条, 验证集100条, 测试集200条。

#### 评测数据量：

评测数据量为公开的测试集200条。

#### 源数据字段：

|  KEYS   | EXPLAIN  |
|  ----  | ----  |
| question  | 问题数据id |
| choices  | 含有四个选项的列表 |
| answer  | 正确答案 |


### 源数据集样例：

```
{
  "question":"以下哪个成语与其他三个表达的意思最不相同？",
  "choices": [
    "身先士卒",
    "一马平川",
    "一马当先",
    "以身作则"
  ],
  "answer":"D"
}
```


### 论文引用：

>   ```
>   @article{hu2022lora,
>     title={Lora: Low-rank adaptation of large language models.},
>     author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
>     journal={ICLR},
>     volume={1},
>     number={2},
>     pages={3},
>     year={2022}
>   }
>   ```


### 源数据集版权使用说明：

