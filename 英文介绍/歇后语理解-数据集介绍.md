
## Two-Part Allegorical Saying Comprehension Dataset  

### Data Description:  
The Two-Part Allegorical Saying Comprehension Dataset is structured as standardized multiple-choice questions, comprising two main categories:  

#### 1. Upper-Lower Sentence Correlation Multiple-Choice Questions  
Questions present the complete structural elements of two-part allegorical sayings:  
- **Deriving the lower sentence from the upper sentence**: Given the first half (metaphorical description) of an allegorical saying (e.g., "Confucius moving house"), examinees must select the logically matching second half (explanatory meaning) from options (e.g., "All books (losses)"). Distractors may include homophone misunderstandings or semantically related but inaccurate expressions.  
- **Deriving the upper sentence from the lower sentence**: Providing the second half of an allegorical saying, examinees must reversely match the corresponding first half. This focuses on testing the accuracy of memory for fixed collocations and the depth of understanding of metaphorical logic. Through structured option design, such questions accurately assess examinees’ mastery of fixed collocations and their ability to analyze semantic mapping relationships between the two parts.  

#### 2. Contextual Application Multiple-Choice Questions  
Questions construct real-life language scenarios with gaps for allegorical sayings at key positions, requiring examinees to select the most context-appropriate option based on:  
- **Semantic fit**: Determining whether the allegorical saying’s meaning aligns with the sentence’s core message (e.g., distinguishing between "Monkey breaking corn" and "墙头草 [墙头草: fence-sitter]" in a context describing "capricious behavior").  
- **Rhetorical appropriateness**: Evaluating whether rhetorical devices (e.g., metaphors, puns) in the allegorical saying match the context’s style (e.g., judging the suitability of colloquial allegorical sayings in formal written contexts).  
- **Cultural metaphor comprehension**: Assessing examinees’ knowledge of cultural allusions behind allegorical sayings (e.g., understanding "Zhou Yu beats Huang Gai — one is willing to beat, the other is willing to endure" requires familiarity with *Romance of the Three Kingdoms*典故 [典故: historical allusions]). These questions deeply evaluate practical application skills through semantic discrimination in specific contexts, covering multi-level competencies from basic memory to contextual transfer.  


### Adaptation Method:  
LoRA Adaptation  
LoRA fine-tuning refers to using low-rank decomposition to represent the parameter updates of large models, thereby reducing the resources and time required for fine-tuning. LoRA is the abbreviation of "Low-Rank Adaptation of Large Language Models," originating from the paper *LoRA: Low-Rank Adaptation of Large Language Models*. The basic idea of LoRA is to assume that the amount of weight change in the model during task adaptation is low-rank. Therefore, parameter updates can be represented by two smaller matrices, while keeping the pre-trained weights unchanged. LoRA can be applied to various natural language processing tasks, such as content understanding and generation tasks. Experiments show that LoRA can significantly reduce training parameters and inference latency while maintaining or improving model performance.


### Dataset Composition and Specifications:  
#### Source Data Volume:  
- Training set: 796 items  
- Validation set: 114 items  
- Test set: 227 items  

#### Evaluation Data Volume:  
Publicly available test set of 227 items.  

#### Source Data Fields:  
|  KEYS   | EXPLAIN  |  
|  ----  | ----  |  
| question  | Question data ID |  
| choices  | List of four options |  
| answer  | Correct answer |  



### Source Dataset Sample:  
```json  
{  
  "question": "____, which means not forcing others but waiting for willing participants or acceptors.",  
  "choices": [  
    "Confucius moving house — all books (losses)",  
    "The flood washes away the Dragon King's temple — family members not recognizing each other",  
    "Jiang Taigong fishing — those who are willing take the bait",  
    "Hanging a sheep's head but selling dog meat — having a reputation but no real substance"  
  ],  
  "answer": "C"  
}  
```


### Paper Citation:  

>   ```
>   @article{hu2022lora,
>     title={Lora: Low-rank adaptation of large language models.},
>     author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
>     journal={ICLR},
>     volume={1},
>     number={2},
>     pages={3},
>     year={2022}
>   }
>   ```


### Source Dataset Copyright Usage Notice:  
[Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License](https://fluent.ai/wp-content/uploads/2021/04/Fluent_Speech_Commands_Public_License.pdf)