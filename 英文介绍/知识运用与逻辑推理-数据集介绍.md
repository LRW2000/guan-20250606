
## Knowledge Application and Logical Reasoning Dataset  

### Data Description:  
The Knowledge Application and Logical Reasoning Dataset includes various question types such as multiple-choice and fill-in-the-blank, with reasoning difficulty levels ranging from 1 to 10 points.  

### Adaptation Method:  
LoRA Adaptation  
LoRA fine-tuning refers to using low-rank decomposition to represent the parameter updates of large models, thereby reducing the resources and time required for fine-tuning. LoRA is the abbreviation of "Low-Rank Adaptation of Large Language Models," originating from the paper *LoRA: Low-Rank Adaptation of Large Language Models*. The basic idea of LoRA is to assume that the amount of weight change in the model during task adaptation is low-rank. Therefore, parameter updates can be represented by two smaller matrices, while keeping the pre-trained weights unchanged. LoRA can be applied to various natural language processing tasks, such as content understanding and generation tasks. Experiments show that LoRA can significantly reduce training parameters and inference latency while maintaining or improving model performance.

### Dataset Composition and Specifications:  
#### Source Data Volume:  
- Training set: 1,400 items  
- Validation set: 200 items  
- Test set: 400 items  

#### Evaluation Data Volume:  
Publicly available test set of 400 items.  

#### Source Data Fields:  
|  KEYS   | EXPLAIN  |  
|  ----  | ----  |  
| question  | Question data |  
| choices  | List of four options |  
| answer  | Correct answer |  


### Source Dataset Sample:  
```json  
{  
  "question": "Xiao Yan, Xiao Yi, and Xiao Kong stood out from their unit and participated in a job competition in the city. Five predictions were made: (1) Both Xiao Yan and Xiao Yi are selected; (2) At most one of Xiao Yan and Xiao Yi is selected; (3) Xiao Yan is selected, but Xiao Yi is not; (4) Xiao Yan is not selected, but Xiao Yi is selected; (5) If Xiao Yan is selected, then Xiao Kong is also selected. It turned out that only one prediction was correct. Which of the following can be inferred? ",  
  "choices": [  
    "Neither Xiao Yan nor Xiao Yi is selected",  
    "Both Xiao Yi and Xiao Kong are selected",  
    "Neither Xiao Yan nor Xiao Kong is selected",  
    "Both Xiao Yan and Xiao Yi are selected"  
  ],  
  "answer": "D"  
}  
```  


### Paper Citation:  

>   ```
>   @article{hu2022lora,
>     title={Lora: Low-rank adaptation of large language models.},
>     author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
>     journal={ICLR},
>     volume={1},
>     number={2},
>     pages={3},
>     year={2022}
>   }
>   ```


### Source Dataset Copyright Usage Notice:  
[Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License](https://fluent.ai/wp-content/uploads/2021/04/Fluent_Speech_Commands_Public_License.pdf)
